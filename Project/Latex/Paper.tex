%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Stylish Article
% LaTeX Template
% Version 2.1 (1/10/15)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Mathias Legrand (legrand.mathias@gmail.com) 
% With extensive modifications by:
% Vel (vel@latextemplates.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[fleqn,12pt]{SelfArx} % Document font size and equations flushed left

\usepackage[english]{babel} % Specify a different language here - english by default

\usepackage{lipsum} % Required to insert dummy text. To be removed otherwise

%----------------------------------------------------------------------------------------
%	COLUMNS
%----------------------------------------------------------------------------------------

\setlength{\columnsep}{0.55cm} % Distance between the two columns of text
\setlength{\fboxrule}{0.75pt} % Width of the border around the abstract

%----------------------------------------------------------------------------------------
%	COLORS
%----------------------------------------------------------------------------------------

\definecolor{color1}{RGB}{0,0,90} % Color of the article title and sections
\definecolor{color2}{RGB}{0,20,20} % Color of the boxes behind the abstract and headings

%----------------------------------------------------------------------------------------
%	HYPERLINKS
%----------------------------------------------------------------------------------------

\usepackage{hyperref} % Required for hyperlinks
\hypersetup{hidelinks,colorlinks,breaklinks=true,urlcolor=color2,citecolor=color1,linkcolor=color1,bookmarksopen=false,pdftitle={Title},pdfauthor={Author}}

%----------------------------------------------------------------------------------------
%	ARTICLE INFORMATION
%----------------------------------------------------------------------------------------

\JournalInfo{6.867 Final Paper} % Journal information
\Archive{} % Additional notes (e.g. copyright, DOI, review/research article)

\PaperTitle{How Does Content Drive Viewership?} % Article title

\Authors{Dave Holtz\textsuperscript{1}, Jeremy Yang\textsuperscript{2}, Michael Zhao\textsuperscript{3}} % Authors
\affiliation{\textsuperscript{1}\textit{dholtz@mit.edu}}
\affiliation{\textsuperscript{2}\textit{zheny@mit.edu}} 
\affiliation{\textsuperscript{3}\textit{mfzhao@mit.edu}}

\Keywords{} % Keywords - if you don't want any simply remove all the text between the curly brackets
\newcommand{\keywordname}{Keywords} % Defines the keywords heading name

%----------------------------------------------------------------------------------------
%	ABSTRACT
%----------------------------------------------------------------------------------------

\Abstract{Why do some webpages receive massive numbers of page views? To determine how content drives viewership, we construct a unique dataset of all articles published by the New York Times (NYT) in August 2013. Our dataset is built from 2 major components, the NYT's internal web traffic data and article content data parsed from the NYT website. We use the internal web traffic data to accurately track the number of page views of each article as well as construct a set of robust control variables such as the desk and section of each article. To build content features, we use various machine learning and statistical natural language processing techniques on our parsed article content data and construct features such as article perplexity, sentiment, reading difficulty, and indicators that denote the presence of pictures, videos, etc. Additionally, we have access to the NYT's internal website traffic data. We feed all of our constructed features to into a predictive regression model. We find [MAJOR RESULTS HERE].
}

%----------------------------------------------------------------------------------------

\begin{document}

\flushbottom % Makes all text pages the same height

\maketitle % Print the title and abstract box

\tableofcontents % Print the contents section

\thispagestyle{empty} % Removes page numbering from the first page

%----------------------------------------------------------------------------------------
%	ARTICLE CONTENTS
%----------------------------------------------------------------------------------------

\section{Introduction} % The \section*{} command stops section numbering

In today's digital economy, many companies are very interested in attracting users to visit their websites in order to earn ad revenue. While many factors might motivate a user to visit a particular page, certainly one important factor is the content in that webpage. This paper explores the relationship between the content of a webpage and the number of page views it ultimately ends up receiving by constructing a unique dataset of all articles published by the New York Times (NYT) during August 2013. This dataset is built from two major components: the NYT's internal web traffic data and parsed NYT article content data.

Typically, a study such as ours tends to be very difficult to conduct as either accurate measures of viewership are unavailable\footnote{While oftentimes precise viewership data tends to be not available openly, oftentimes researchers use related observables, such as Facebook likes} or the feature extraction of the content is too challenging (for example Youtube), or or both. Fortunately, our access to the the NYT's internal web traffic data allows us to exactly measure the number of page views an article receives. The web traffic data is rather rich and also includes internal meta-data that we use to build various control features. Moreover, since we are working with mostly textual data, we are able to take advantage of recent advancements in machine learning and statistical NLP to do feature extraction on article text. 

A similar study by Berger and Milkman (2012) \cite{berger2012makes} examines the relationship between content and word-of-mouth virality. They find that the emotional content of a NYT article is predictive of its virality. Using simple measures of an article's sentiment and emotionality, Berger and Milkman show that positive articles are more likely to show up on the New York Times "Most-Emailed" list. They also show that articles that evoke high physiological positive or negative arousal (such as awe or anger) tend to be more viral than articles that evoke deactivating emotions (sadness). We build on this study in two ways: first, we relate an article's content back to the number of page views it receives rather than its virality\footnote{Which companies arguably care more about since word-of-mouth virality is usually a means to increase page views}. Second, we employ more sophisticated machine learning feature extraction techniques to see if they work any better over their simple measures.
%------------------------------------------------

\section{Data}
\subsection{NYT Internal Web Traffic Data}
Our NYT internal web traffic dataset is a record of all individual user activity on the NYT website covering the period of April 3rd, 2013 to October 31st, 2013. This activity data is stored as individual lines of json and includes who (if available) accessed what page at what time. Overall, it is over 20 terabytes in size and contains over 3 billion page views\footnote{Not all page views are content views, for example, some events that are also tracked are searches, or user account settings.} Since the scope of this dataset is so large, we initially restrict this project to a single month, August 2013. 

We limit our dataset to consist of pages that only contain articles or blogposts published during the month of August. We parse the data to obtain a list of urls, which need to be stripped of potential garbage. After cleaning up the url data, we are left with 6682 unique pieces of content. We then parse our dataset and aggregate the number of counts each url receives. In order to make the comparison between articles fairer since an article that's been out longer will have more page views on average, we only count the number of page views received up to 7 days after publication\footnote{Given that page views tend to sharply drop off soon after publication since recency is quite important to the News, the number of page views obtained during the 7 days after an article is published represents the vast majority (usually well over 90\%) of total page views an article receives.}. In total, our data consists of over 250 million page views. As seen in Figure 1 below, the distribution of page views is highly skewed with very heavy tails. After applying a log transformation (as seen in Figure 2), our distribution looks considerably more normal. 

\begin{figure}[ht]\centering
\includegraphics[width=\linewidth]{pageviews_hist}
\caption{Histogram of Articles by Number of Page Views}
\label{fig:pv_hist}
\end{figure}
% TWEAK TABLE LOOK
\begin{table}[hbt]
\caption{Page Views Distribution Summary Statistics}
\centering
\begin{tabular}{rr}
\toprule
Total Page Views  &  248161455\\
\midrule
Min               &  1\\
Max               &  2545288\\  
Mean              &  37138.8\\
Median            &  10298.5\\
Std. Dev.         &  88972.9\\
Skewness          &  9.52191\\
Kurtosis          &  173.061\\
\midrule
Observations &  6682\\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[ht]\centering
\includegraphics[width=\linewidth]{log_pageviews_hist}
\caption{Histogram of Articles by Log of Page Views}
\label{fig:lpv_hist}
\end{figure}

\begin{table}[hbt]
\caption{Log Page Views Distribution Summary Statistics}
\centering
\begin{tabular}{rr}
\toprule
Min               &  0\\
Max               &  14.74975\\  
Mean              &  9.122868\\
Median            &  9.239754\\
Std. Dev.         &  2.028668\\
Skewness          &  -1.270368\\
Kurtosis          &  3.800911\\
\midrule
Observations &  6682\\
\bottomrule
\end{tabular}
\end{table}


In addition to aggregating the counts, when 

\subsection{Parsed NYT Article Content Data}

\begin{figure}[ht]\centering
\includegraphics[width=\linewidth]{wordcount_hist}
\caption{Histogram of Articles by Word Count}
\label{fig:pv_hist}
\end{figure}

\begin{figure}[ht]\centering
\includegraphics[width=\linewidth]{log_wordcount_hist}
\caption{Histogram of Articles by Log Word Count}
\label{fig:lpv_hist}
\end{figure}

%\begin{figure*}[ht]\centering % Using \begin{figure*} makes the figure take up the entire width of the page
%\includegraphics[width=\linewidth]{view}
%\caption{Wide Picture}
%\label{fig:view}
%\end{figure*}

%------------------------------------------------

\section{Constructed Features}

In preparing to perform our regression, we construct a number of features, using both the parsed NYT article data and secondary sources. These features include the Flesch reading ease, the (guessed) gender of the author(s), the popularity of the author(s), the sentiment of the article text, the perplexity of the article text, and variables indicating the section the article appeared in and the article's content type. Found below is a full list of these features, as well as the methodology used to construct them and validation of the features, where appropriate.

\begin{description}
	\item[Flesch Reading Ease] The Flesch reading ease is a metric developed by Flesch in 1948 \cite{flesch1948new}. The score indicates how difficult a piece of English text is to understand. Lower scores correspond to more difficult passages, and the highest score attainable is 120.0. The formula for calculating a passage's Flesch reading ease is
	
	\begin{equation}
	206.835 - 1.015 \left ( \frac{\textrm{\# words}}{\textrm{\# sentences}} \right ) - 84.6 \left ( \frac{\textrm{\# syllables}}{\textrm{\# words}} \right )
	\end{equation}
	
	To calculate the Flesch reading ease, we use the python library ``textstat." Despite the fact that the above formula is relatively straightforward, the task of counting the number of syllables in a block of text is non-trivial, so we rely on ``textstat" to do so accurately. In cases where the Flesch reading ease was for some reason null (e.g., a blog post containing only a picture), we assign the Flesch reading ease its median value.
	
	\item[Author Popularity] We attempt to include some measure of a particular author's popularity. It stands to reason that a new article by Paul Krugman or A.O. Scott should garner more readership than a new article by an unknown graduate student enrolled in 6.867 at MIT! 
	
	In order to measure something that will serve as a decent proxy for popularity, we programmatically searched for every distinct author in our dataset using Bing, and recorded the number of search results that were returned by the query. In cases where a particular article has more than one distinct author, we calculate an ``effective" popularity by simply calculating the average number of search results over all article authors.
	
	\item[Author Gender] We also attempt to construct a feature that indicates the most likely gender of the article author(s). In cases where the gender of the author is unclear (e.g., Robin) or there are likely multiple authors with different genders (e.g., The New York Times Staff), we record a third gender value, ``ambiguous / unknown." 
	
	We construct our gender data by cross-referencing the first names of all of the authors in our dataset against U.S. Social Security Administration baby name data from 1935 to 1997. If over 90\% of the babies with a given name have been male, we assume a given author is male. If over 90\% of the babies with a given name have been female, we assume a given author is female. Otherwise, we record ``ambiguous / unknown."
	
	\item[Material Type, Section, Desk, and Article Type] We also include dummy variables including the material type (e.g., `News' or `Obituary'), desk (e.g., `Weekend' or `Real Estate'), article type (`Blog post' or `Article'), and section (e.g., `Movies' or `World').
	
\end{description}

We also include features that attempt to capture the article sentiment and the article text perplexity. Since the construction of these features was more involved and involved validation of our algorithms, we discuss these two features in separate subsections.

\subsection{Article Sentiment}

\subsection{Article Perplexity}

\section{Predictive Regression Model}

\begin{figure*}[ht]\centering
\includegraphics[width=\linewidth]{feature_weights.png}
\caption{Linear Regression Feature Weights (excluding intercept term)}
\label{fig:lin_reg_weights}
\end{figure*}

\begin{figure}[ht]\centering
\includegraphics[width=\linewidth]{mse_plot.png}
\caption{The effect of regularization on MSE}
\label{fig:mse_plot}
\end{figure}

\begin{center}
\begin{tabular}{lr}
\toprule
Feature  &  Weight\\
\midrule
intercept & 9.047\\
log\_wcount & 0.729\\
desk\_Foreign & 0.657\\
section\_World & -0.553\\
desk\_Travel & -0.472\\
typeOfMaterial\_Schedule & -0.291\\
section\_Movies & -0.277\\
section\_Opinion & 0.261\\
typeOfMaterial\_Review & -0.255\\
desk\_National & 0.253\\
typeOfMaterial\_Letter & -0.243\\
section\_Sports & -0.215\\
section\_Education & -0.199\\
section\_Books & 0.192\\
section\_Corrections & -0.179\\
desk\_Weekend & -0.175\\
section\_Health & 0.172\\
type\_BlogPost & -0.170\\
desk\_BookReview & -0.169\\
typeOfMaterial\_Op-Ed & 0.167\\
\end{tabular}
\end{center}
%------------------------------------------------
\phantomsection
\section*{Acknowledgments} % The \section*{} command stops section numbering

\addcontentsline{toc}{section}{Acknowledgments} % Adds this section to the table of contents

%----------------------------------------------------------------------------------------
%	REFERENCE LIST
%----------------------------------------------------------------------------------------
\phantomsection
\bibliographystyle{unsrt}
\bibliography{Paper}

%----------------------------------------------------------------------------------------

\end{document}